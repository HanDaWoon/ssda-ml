{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msconf\u001b[39;00m \u001b[39mimport\u001b[39;00m Config\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdmfont\u001b[39;00m \u001b[39mimport\u001b[39;00m utils\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdmfont\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdmfont\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m MACore\n",
      "File \u001b[0;32m~/CHJ/ml/FastAPI/app/dmfont/__init__.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39msys\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mevaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/CHJ/ml/FastAPI/app/dmfont/evaluator.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msconf\u001b[39;00m \u001b[39mimport\u001b[39;00m Config\n\u001b[0;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mutils\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m  \u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m  \u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m MACore\n",
      "File \u001b[0;32m~/CHJ/ml/FastAPI/app/utils/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpng2svg\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpng2svgs\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmake_font\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/CHJ/ml/FastAPI/app/utils/make_font.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m transforms\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdmfont\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m MACore\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdmfont\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluator\u001b[39;00m \u001b[39mimport\u001b[39;00m Evaluator\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdmfont\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger\n",
      "File \u001b[0;32m~/CHJ/ml/FastAPI/app/dmfont/models/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mDMFont\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mCopyright (c) 2020-present NAVER Corp.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mMIT license\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mma_core\u001b[39;00m \u001b[39mimport\u001b[39;00m MACore\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdiscriminator\u001b[39;00m \u001b[39mimport\u001b[39;00m Discriminator\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39maux_classifier\u001b[39;00m \u001b[39mimport\u001b[39;00m AuxClassifier\n",
      "File \u001b[0;32m~/CHJ/ml/FastAPI/app/dmfont/models/ma_core.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcomp_encoder\u001b[39;00m \u001b[39mimport\u001b[39;00m ComponentEncoder\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdecoder\u001b[39;00m \u001b[39mimport\u001b[39;00m Decoder\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmemory\u001b[39;00m \u001b[39mimport\u001b[39;00m Memory\n\u001b[1;32m     12\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMACore\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     13\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Memory-augmented HFG \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/CHJ/ml/FastAPI/app/dmfont/models/memory.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m  \u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m split_dim, ConvBlock\n\u001b[0;32m---> 10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkor_decompose\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mkor\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mthai_decompose\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mthai\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcomp_id_to_addr\u001b[39m(ids, language):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from sconf import Config\n",
    "\n",
    "from dmfont import utils\n",
    "from dmfont.logger import Logger\n",
    "\n",
    "from dmfont.models import MACore\n",
    "from dmfont.datasets import uniform_sample\n",
    "from dmfont.datasets import kor_decompose as kor\n",
    "from dmfont.datasets import thai_decompose as thai\n",
    "from dmfont.inference import (\n",
    "    infer, get_val_loader,\n",
    "    infer_2stage, get_val_encode_loader, get_val_decode_loader\n",
    ")\n",
    "from dmfont.ssim import SSIM, MSSSIM\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_to_hdf5(dump_path, font_name, images, chars, compression=None):\n",
    "    with h5.File(dump_path, 'w') as f:\n",
    "        dset = f.create_group('dataset')\n",
    "        dset.attrs['font_name'] = font_name\n",
    "        N = len(images)\n",
    "        dset.create_dataset('images', (N, 128, 128), np.uint8, compression=compression,\n",
    "                            data=np.stack(images))\n",
    "        data = np.array(chars)\n",
    "        print(data.shape, -\"data\")\n",
    "        dset.create_dataset('chars', data.shape, np.int32, compression=compression,\n",
    "                            data=np.array(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import h5py as h5\n",
    "import fire\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont, features\n",
    "from fontTools.ttLib import TTFont\n",
    "\n",
    "from glob import glob\n",
    "import unicodedata\n",
    "# from logger import Logger\n",
    "# from datasets import thai_decompose as thai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<PIL.Image.Image image mode=L size=128x128 at 0x7F8CF4713820>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3EABE20>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CF4713D60>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CF4713670>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CF4713C40>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CF4713D30>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3EBC220>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CF4713CD0>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3EBCD30>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3EBC4F0>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3EBCDC0>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49730>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E497F0>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49790>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49820>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49880>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E498B0>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E498E0>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49910>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49940>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49970>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E499A0>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E499D0>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49A00>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49A30>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49A60>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49A90>, <PIL.Image.Image image mode=L size=128x128 at 0x7F8CA3E49AC0>] [44032, 44583, 44649, 45200, 45266, 45845, 45883, 46462, 46500, 47079, 47117, 47696, 47734, 48313, 48351, 48968, 49518, 49585, 49614, 50135, 50819, 51436, 52053, 52670, 53287, 53904, 54521, 55138]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "font_name = \"CHJ\"\n",
    "dump_dir = Path(f\"/home/software/CHJ/ml/FastAPI/app/dmfont/dump_dir/{font_name}\")\n",
    "dump_dir.mkdir(parents=True, exist_ok=True)\n",
    "hdf5_name = \"{}.hdf5\".format(font_name, font_name)\n",
    "dump_path = dump_dir / hdf5_name\n",
    "\n",
    "# if dump_path.exists():\n",
    "#     print(\"exits\")\n",
    "# else:\n",
    "images = []\n",
    "chars = []\n",
    "for image_path in sorted(glob(os.path.join(f\"/home/software/CHJ/ml/FastAPI/app/dmfont/font_images/{font_name}\", \"*.png\"))):\n",
    "    image = Image.open(image_path).convert(\"L\")\n",
    "    image = image.resize((128, 128))\n",
    "    char = unicodedata.normalize('NFC', os.path.basename(image_path).split(\".\")[0]) # ~~~/가\n",
    "    # print(char, \"-char 이름\")\n",
    "    # print(image.size)\n",
    "    images.append(image)\n",
    "    chars.append(ord(char))\n",
    "print(images, chars)\n",
    "with h5.File(dump_path, 'w') as f:\n",
    "    dset = f.create_group('dataset')\n",
    "    dset.attrs['font_name'] = font_name\n",
    "    \n",
    "    N = len(images)\n",
    "    M = len(chars)\n",
    "    st = np.stack(images)\n",
    "    # print(N, M, st.shape, \"-N, M, st.shape\")\n",
    "    dset.create_dataset('images', (N, 128, 128), np.uint8, compression=None, data=np.stack(images))\n",
    "    data = np.array(chars)\n",
    "    dset.create_dataset('chars', data.shape, np.int32, compression=None, data=np.array(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: MaHFG-eval [-h] [--show] [--mode MODE] [--deterministic] [--debug]\n",
      "                  name resume img_dir config_paths [config_paths ...]\n",
      "MaHFG-eval: error: the following arguments are required: name, resume, img_dir, config_paths\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/software/.conda/envs/ML/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DMFont\n",
    "Copyright (c) 2020-present NAVER Corp.\n",
    "MIT license\n",
    "\"\"\"\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from sconf import Config\n",
    "\n",
    "from dmfont import utils\n",
    "from dmfont.logger import Logger\n",
    "\n",
    "from dmfont.models import MACore\n",
    "from dmfont.datasets import uniform_sample\n",
    "from dmfont.datasets import kor_decompose as kor\n",
    "from dmfont.datasets import thai_decompose as thai\n",
    "from dmfont.inference import (\n",
    "    infer, get_val_loader,\n",
    "    infer_2stage, get_val_encode_loader, get_val_decode_loader\n",
    ")\n",
    "from dmfont.ssim import SSIM, MSSSIM\n",
    "\n",
    "\n",
    "def torch_eval(val_fn):\n",
    "    @torch.no_grad()\n",
    "    def decorated(self, gen, *args, **kwargs):\n",
    "        gen.eval()\n",
    "        ret = val_fn(self, gen, *args, **kwargs)\n",
    "        gen.train()\n",
    "\n",
    "        return ret\n",
    "\n",
    "    return decorated\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"DMFont evaluator.\n",
    "    The evaluator provides pixel-level evaluation and glyphs generation\n",
    "    from the reference style samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, trn_avails, logger, writer, batch_size, transform,\n",
    "                 content_font, language, meta, val_loaders, n_workers=2):\n",
    "        self.data = data\n",
    "        self.logger = logger\n",
    "        self.writer = writer\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.n_workers = n_workers\n",
    "        self.unify_resize_method = True\n",
    "\n",
    "        self.trn_avails = trn_avails\n",
    "        self.val_loaders = val_loaders\n",
    "        self.content_font = content_font\n",
    "        self.language = language\n",
    "        if self.language == 'kor':\n",
    "            self.n_comp_types = 3\n",
    "        elif self.language == 'thai':\n",
    "            self.n_comp_types = 4\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        # setup cross-validation\n",
    "        self.SSIM = SSIM().cuda()\n",
    "        weights = [0.25, 0.3, 0.3, 0.15]\n",
    "        self.MSSSIM = MSSSIM(weights=weights).cuda()\n",
    "\n",
    "        n_batches = [len(loader) for loader in self.val_loaders.values()]\n",
    "        self.n_cv_batches = min(n_batches)\n",
    "        self.logger.info(\"# of cross-validation batches = {}\".format(self.n_cv_batches))\n",
    "\n",
    "        # the number of chars/fonts for CV visualization\n",
    "        n_chars = 16\n",
    "        n_fonts = 16\n",
    "        seen_chars = uniform_sample(meta['train']['chars'], n_chars//2)\n",
    "        unseen_chars = uniform_sample(meta['valid']['chars'], n_chars//2)\n",
    "        unseen_fonts = uniform_sample(meta['valid']['fonts'], n_fonts)\n",
    "\n",
    "        self.cv_comparable_fonts = unseen_fonts\n",
    "        self.cv_comparable_chars = seen_chars + unseen_chars\n",
    "\n",
    "        allchars = meta['train']['chars'] + meta['valid']['chars']\n",
    "        self.cv_comparable_avails = {\n",
    "            font: allchars\n",
    "            for font in self.cv_comparable_fonts\n",
    "        }\n",
    "\n",
    "    def validation(self, gen, step, extra_tag=''):\n",
    "        self.comparable_validset_validation(gen, step, True, 'comparable_val'+extra_tag)\n",
    "\n",
    "        plot_dic = {}\n",
    "        for tag, loader in self.val_loaders.items():\n",
    "            tag = tag + extra_tag\n",
    "            l1, ssim, msssim = self.cross_validation(\n",
    "                gen, step, loader, tag, n_batches=self.n_cv_batches\n",
    "            )\n",
    "            plot_dic[f'val/{tag}/l1'] = l1\n",
    "            plot_dic[f'val/{tag}/ssim'] = ssim\n",
    "            plot_dic[f'val/{tag}/ms-ssim'] = msssim if not np.isnan(msssim) else 0.\n",
    "        self.writer.add_scalars(plot_dic, step)\n",
    "\n",
    "        return plot_dic\n",
    "\n",
    "    @torch_eval\n",
    "    def comparable_validset_validation(self, gen, step, compare_inputs=False, tag='comparable_val'):\n",
    "        \"\"\"Comparable validation on validation set from CV\"\"\"\n",
    "        comparable_grid = self.comparable_validation(\n",
    "            gen, self.cv_comparable_avails, self.cv_comparable_fonts, self.cv_comparable_chars,\n",
    "            n_max_match=1, compare_inputs=compare_inputs\n",
    "        )\n",
    "\n",
    "        self.writer.add_image(tag, comparable_grid, global_step=step)\n",
    "\n",
    "    @torch_eval\n",
    "    def comparable_validation(self, gen, style_avails, target_fonts, target_chars, n_max_match=3,\n",
    "                              compare_inputs=False):\n",
    "        \"\"\"Compare horizontally for target fonts and chars\"\"\"\n",
    "        # infer\n",
    "        loader = get_val_loader(\n",
    "            self.data, target_fonts, target_chars, style_avails,\n",
    "            B=self.batch_size, n_max_match=n_max_match, transform=self.transform,\n",
    "            content_font=self.content_font, language=self.language, n_workers=self.n_workers\n",
    "        )\n",
    "        print(self.batch_size, \"BS\")\n",
    "        out = infer(gen, loader)  # [B, 1, 128, 128]\n",
    "\n",
    "        # ref original chars\n",
    "        refs = self.get_charimages(target_fonts, target_chars)\n",
    "        # print(refs.shape, \"refs.shape\")\n",
    "        # print(out.shape, \"out.shape\")\n",
    "        compare_batches = [refs, out]\n",
    "        # print(np.shape(np.array(compare_batches)), \"compare_batches.shape\")\n",
    "        if compare_inputs:\n",
    "            compare_batches += self.get_inputimages(loader)\n",
    "\n",
    "        nrow = len(target_chars)\n",
    "        comparable_grid = utils.make_comparable_grid(*compare_batches, nrow=nrow)\n",
    "\n",
    "        return comparable_grid\n",
    "\n",
    "    @torch_eval\n",
    "    def cross_validation(self, gen, step, loader, tag, n_batches, n_log=64, save_dir=None):\n",
    "        \"\"\"Validation using splitted cross-validation set\n",
    "        Args:\n",
    "            n_log: # of images to log\n",
    "            save_dir: if given, images are saved to save_dir\n",
    "        \"\"\"\n",
    "        if save_dir:\n",
    "            save_dir = Path(save_dir)\n",
    "            save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        outs = []\n",
    "        trgs = []\n",
    "        n_accum = 0\n",
    "\n",
    "        losses = utils.AverageMeters(\"l1\", \"ssim\", \"msssim\")\n",
    "        for i, (style_ids, style_comp_ids, style_imgs,\n",
    "                trg_ids, trg_comp_ids, content_imgs, trg_imgs) in enumerate(loader):\n",
    "            if i == n_batches:\n",
    "                break\n",
    "\n",
    "            style_ids = style_ids.cuda()\n",
    "            style_comp_ids = style_comp_ids.cuda()\n",
    "            style_imgs = style_imgs.cuda()\n",
    "            trg_ids = trg_ids.cuda()\n",
    "            trg_comp_ids = trg_comp_ids.cuda()\n",
    "            trg_imgs = trg_imgs.cuda()\n",
    "\n",
    "            gen.encode_write(style_ids, style_comp_ids, style_imgs)\n",
    "            out = gen.read_decode(trg_ids, trg_comp_ids)\n",
    "            B = len(out)\n",
    "\n",
    "            # log images\n",
    "            if n_accum < n_log:\n",
    "                trgs.append(trg_imgs)\n",
    "                outs.append(out)\n",
    "                n_accum += B\n",
    "\n",
    "                if n_accum >= n_log:\n",
    "                    # log results\n",
    "                    outs = torch.cat(outs)[:n_log]\n",
    "                    trgs = torch.cat(trgs)[:n_log]\n",
    "                    self.merge_and_log_image(tag, outs, trgs, step)\n",
    "\n",
    "            l1, ssim, msssim = self.get_pixel_losses(out, trg_imgs, self.unify_resize_method)\n",
    "            losses.updates({\n",
    "                \"l1\": l1.item(),\n",
    "                \"ssim\": ssim.item(),\n",
    "                \"msssim\": msssim.item()\n",
    "            }, B)\n",
    "\n",
    "            # save images\n",
    "            if save_dir:\n",
    "                font_ids = trg_ids.detach().cpu().numpy()\n",
    "                images = out.detach().cpu()  # [B, 1, 128, 128]\n",
    "                char_comp_ids = trg_comp_ids.detach().cpu().numpy()  # [B, n_comp_types]\n",
    "                for font_id, image, comp_ids in zip(font_ids, images, char_comp_ids):\n",
    "                    font_name = loader.dataset.fonts[font_id]  # name.ttf\n",
    "                    font_name = Path(font_name).stem  # remove ext\n",
    "                    (save_dir / font_name).mkdir(parents=True, exist_ok=True)\n",
    "                    if self.language == 'kor':\n",
    "                        char = kor.compose(*comp_ids)\n",
    "                    elif self.language == 'thai':\n",
    "                        char = thai.compose_ids(*comp_ids)\n",
    "\n",
    "                    uni = \"\".join([f'{ord(each):04X}' for each in char])\n",
    "                    path = save_dir / font_name / \"{}_{}.png\".format(font_name, uni)\n",
    "                    utils.save_tensor_to_image(image, path)\n",
    "\n",
    "        self.logger.info(\n",
    "            \"  [Valid] {tag:30s} | Step {step:7d}  L1 {L.l1.avg:7.4f}  SSIM {L.ssim.avg:7.4f}\"\n",
    "            \"  MSSSIM {L.msssim.avg:7.4f}\"\n",
    "            .format(tag=tag, step=step, L=losses))\n",
    "\n",
    "        return losses.l1.avg, losses.ssim.avg, losses.msssim.avg\n",
    "\n",
    "    def get_pixel_losses(self, out, trg_imgs, unify):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            out: generated images\n",
    "            trg_imgs: target GT images\n",
    "            unify: if True is given, unify glyph size and resize method before evaluation.\n",
    "                This option give us the fair evaluation setting, which is used in the paper.\n",
    "        \"\"\"\n",
    "        def unify_resize_method(img):\n",
    "            # Unify various glyph size and resize method for fair evaluation\n",
    "            size = img.size(-1)\n",
    "            if size == 128:\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.Resize([64, 64]),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,))\n",
    "                ])\n",
    "                img = torch.stack([transform(_img) for _img in img.cpu()]).cuda()\n",
    "\n",
    "            img = F.interpolate(img, scale_factor=2.0, mode='bicubic', align_corners=True)\n",
    "            return img\n",
    "\n",
    "        if unify:\n",
    "            out = unify_resize_method(out)\n",
    "            trg_imgs = unify_resize_method(trg_imgs)\n",
    "\n",
    "        l1 = F.l1_loss(out, trg_imgs)\n",
    "        ssim = self.SSIM(out, trg_imgs)\n",
    "        msssim = self.MSSSIM(out, trg_imgs)\n",
    "\n",
    "        return l1, ssim, msssim\n",
    "\n",
    "    @torch_eval\n",
    "    def handwritten_validation_2stage(self, gen, step, fonts, style_chars, target_chars,\n",
    "                                      comparable=False, save_dir=None, tag='hw_validation_2stage'):\n",
    "        \"\"\"2-stage handwritten validation\n",
    "        Args:\n",
    "            fonts: [font_name1, font_name2, ...]\n",
    "            save_dir: if given, do not write image grid, instead save every image into save_dir\n",
    "        \"\"\"\n",
    "        if save_dir is not None:\n",
    "            save_dir = Path(save_dir)\n",
    "            save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        outs = []\n",
    "        # for font_name in tqdm(fonts):\n",
    "        for font_name in fonts:\n",
    "            print(font_name)\n",
    "            encode_loader = get_val_encode_loader(\n",
    "                self.data, font_name, style_chars, self.language, self.transform\n",
    "            )\n",
    "            decode_loader = get_val_decode_loader(target_chars, self.language)\n",
    "            out = infer_2stage(gen, encode_loader, decode_loader)\n",
    "            outs.append(out)\n",
    "\n",
    "            if save_dir:\n",
    "                for char, glyph in zip(target_chars, out):\n",
    "                    uni = \"\".join([f'{ord(each):04X}' for each in char])\n",
    "                    path = save_dir / font_name / \"{}_{}.png\".format(font_name, uni)\n",
    "                    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    utils.save_tensor_to_image(glyph, path)\n",
    "\n",
    "        # if save_dir:  # do not write grid\n",
    "        #     return\n",
    "\n",
    "        # out = torch.cat(outs)\n",
    "        # if comparable:\n",
    "        #     # ref original chars\n",
    "        #     refs = self.get_charimages(fonts, target_chars)\n",
    "\n",
    "        #     nrow = len(target_chars)\n",
    "        #     grid = utils.make_comparable_grid(refs, out, nrow=nrow)\n",
    "        # else:\n",
    "        #     grid = utils.to_grid(out, 'torch', nrow=len(target_chars))\n",
    "\n",
    "        # tag = tag + target_chars[:4]\n",
    "        # self.writer.add_image(tag, grid, global_step=step)\n",
    "\n",
    "    def get_inputimages(self, val_loader):\n",
    "        # integrate style images\n",
    "        inputs = []\n",
    "        for style_ids, style_comp_ids, style_imgs, trg_ids, trg_comp_ids, content_imgs \\\n",
    "                in val_loader:\n",
    "            inputs.append(style_imgs)\n",
    "\n",
    "        inputs = torch.cat(inputs)\n",
    "        shape = inputs.shape\n",
    "        inputs = inputs.view(shape[0]//self.n_comp_types, self.n_comp_types, *shape[1:])\n",
    "        batches = [inputs[:, i] for i in range(self.n_comp_types)]\n",
    "\n",
    "        return batches\n",
    "\n",
    "    def get_charimages(self, fonts, chars, empty_header=False, as_tensor=True):\n",
    "        \"\"\" get char images from self.data\n",
    "        Return:\n",
    "            2d list of charimages or 5d tensor:\n",
    "            [\n",
    "                [charimage1, charimage2, ...] (font1),\n",
    "                ...\n",
    "            ]\n",
    "            or\n",
    "            Tensor [n_fonts, n_chars, 1, 128, 128]\n",
    "        \"\"\"\n",
    "        empty_box = torch.ones(1, 128, 128)\n",
    "        charimages = [\n",
    "            [self.data.get(font_name, char, empty_box) for char in chars]\n",
    "            for font_name in fonts\n",
    "        ]\n",
    "\n",
    "        if empty_header:\n",
    "            header = [empty_box for _ in chars]\n",
    "            charimages.insert(0, header)\n",
    "\n",
    "        if as_tensor:\n",
    "            charimages = torch.stack(list(chain.from_iterable(charimages)))\n",
    "\n",
    "        return charimages\n",
    "\n",
    "    def merge_and_log_image(self, name, out, target, step):\n",
    "        \"\"\" Merge out and target into 2-column grid and log it \"\"\"\n",
    "        merge = utils.make_merged_grid([out, target], merge_dim=2)\n",
    "        self.writer.add_image(name, merge, global_step=step)\n",
    "\n",
    "\n",
    "def eval_ckpt():\n",
    "    from dmfont.train import (\n",
    "        setup_language_dependent, setup_data, setup_cv_dset_loader,\n",
    "        get_dset_loader\n",
    "    )\n",
    "\n",
    "    logger = Logger.get()\n",
    "\n",
    "    parser = argparse.ArgumentParser('MaHFG-eval')\n",
    "    parser.add_argument(\n",
    "        \"name\", help=\"name is used for directory name of the user-study generation results\"\n",
    "    )\n",
    "    parser.add_argument(\"resume\", default=\"/home/software/CHJ/ml/FastAPI/app/dmfont/checkpoint/korean-handwriting.pth\")\n",
    "    parser.add_argument(\"img_dir\", default=\"/home/software/CHJ/ml/FastAPI/app/dmfont/font_images/CHJ\")\n",
    "    parser.add_argument(\"config_paths\", default=\"/home/software/CHJ/ml/FastAPI/app/dmfont/cfgs/kor_custom.yaml\", nargs=\"+\")\n",
    "    parser.add_argument(\"--show\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\n",
    "        \"--mode\", default=\"user-study-save\",\n",
    "        help=\"eval (default) / cv-save / user-study / user-study-save. \"\n",
    "             \"`eval` generates comparable grid and computes pixel-level CV scores. \"\n",
    "             \"`cv-save` generates and saves all target characters in CV. \"\n",
    "             \"`user-study` generates comparable grid for the ramdomly sampled target characters. \"\n",
    "             \"`user-study-save` generates and saves all target characters in user-study.\"\n",
    "    )\n",
    "    parser.add_argument(\"--deterministic\", default=False, action=\"store_true\")\n",
    "    parser.add_argument(\"--debug\", default=False, action=\"store_true\")\n",
    "    args, left_argv = parser.parse_known_args()\n",
    "    args, left_argv = parser.parse_known_args()\n",
    "\n",
    "    cfg = Config(*args.config_paths)\n",
    "    cfg.argv_update(left_argv)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    cfg['data_dir'] = Path(cfg['data_dir'])\n",
    "\n",
    "    if args.show:\n",
    "        exit()\n",
    "\n",
    "    # seed\n",
    "    np.random.seed(cfg['seed'])\n",
    "    torch.manual_seed(cfg['seed'])\n",
    "    random.seed(cfg['seed'])\n",
    "\n",
    "    if args.deterministic:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        cfg['n_workers'] = 0\n",
    "        logger.info(\"#\" * 80)\n",
    "        logger.info(\"# Deterministic option is activated !\")\n",
    "        logger.info(\"# Deterministic evaluator only ensure the deterministic cross-validation\")\n",
    "        logger.info(\"#\" * 80)\n",
    "    else:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    if args.mode.startswith('mix'):\n",
    "        assert cfg['g_args']['style_enc']['use'], \\\n",
    "                \"Style mixing is only available with style encoder model\"\n",
    "\n",
    "    #####################################\n",
    "    # Dataset\n",
    "    ####################################\n",
    "    # setup language dependent values\n",
    "    content_font, n_comp_types, n_comps = setup_language_dependent(cfg)\n",
    "\n",
    "    # setup transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "    # setup data\n",
    "    hdf5_data, meta = setup_data(cfg, transform)\n",
    "\n",
    "    # setup dataset\n",
    "    trn_dset, loader = get_dset_loader(\n",
    "        hdf5_data, meta['train']['fonts'], meta['train']['chars'], transform, True, cfg,\n",
    "        content_font=content_font\n",
    "    )\n",
    "    # TODO : val_loader 역할?\n",
    "    val_loaders = setup_cv_dset_loader(\n",
    "        hdf5_data, meta, transform, n_comp_types, content_font, cfg\n",
    "    )\n",
    "\n",
    "    #####################################\n",
    "    # Model\n",
    "    ####################################\n",
    "    # setup generator only\n",
    "    g_kwargs = cfg.get('g_args', {})\n",
    "    gen = MACore(\n",
    "        1, cfg['C'], 1, **g_kwargs, n_comps=n_comps, n_comp_types=n_comp_types,\n",
    "        language=cfg['language']\n",
    "    )\n",
    "    gen.cuda()\n",
    "\n",
    "    ckpt = torch.load(args.resume)\n",
    "    logger.info(\"Use EMA generator as default\")\n",
    "    gen.load_state_dict(ckpt['generator_ema'])\n",
    "\n",
    "    step = ckpt['epoch']\n",
    "    loss = ckpt['loss']\n",
    "\n",
    "    logger.info(\"Resumed checkpoint from {} (Step {}, Loss {:7.3f})\".format(\n",
    "        args.resume, step, loss))\n",
    "\n",
    "    writer = utils.DiskWriter(args.img_dir, 0.6)\n",
    "\n",
    "    evaluator = Evaluator(\n",
    "        hdf5_data, trn_dset.avails, logger, writer, cfg['batch_size'],\n",
    "        content_font=content_font, transform=transform, language=cfg['language'],\n",
    "        val_loaders=val_loaders, meta=meta\n",
    "    )\n",
    "    evaluator.n_cv_batches = -1\n",
    "    logger.info(\"Update n_cv_batches = -1 to evaluate about full data\")\n",
    "    if args.debug:\n",
    "        evaluator.n_cv_batches = 10\n",
    "        logger.info(\"!!! DEBUG MODE: n_cv_batches = 10 !!!\")\n",
    "\n",
    "    if args.mode == 'eval':\n",
    "        logger.info(\"Start validation ...\")\n",
    "        dic = evaluator.validation(gen, step)\n",
    "        logger.info(\"Validation is done. Result images are saved to {}\".format(args.img_dir))\n",
    "    elif args.mode.startswith('user-study'):\n",
    "        meta = json.load(open('meta/kor-unrefined_custom.json'))\n",
    "        target_chars = meta['target_chars']\n",
    "        style_chars = meta['style_chars']\n",
    "        fonts = meta['fonts']\n",
    "\n",
    "        if args.mode == 'user-study':\n",
    "            sampled_target_chars = uniform_sample(target_chars, 20)\n",
    "            logger.info(\"Start generation kor-unrefined ...\")\n",
    "            logger.info(\"Sampled chars = {}\".format(sampled_target_chars))\n",
    "\n",
    "            evaluator.handwritten_validation_2stage(\n",
    "                gen, step, fonts, style_chars, sampled_target_chars,\n",
    "                comparable=True, tag='userstudy-{}'.format(args.name)\n",
    "            )\n",
    "        elif args.mode == 'user-study-save':\n",
    "            logger.info(\"Start generation & saving kor-unrefined ...\")\n",
    "            save_dir = Path(args.img_dir) / \"{}-{}\".format(args.name, step)\n",
    "            evaluator.handwritten_validation_2stage(\n",
    "                gen, step, fonts, style_chars, target_chars,\n",
    "                comparable=True, save_dir=save_dir\n",
    "            )\n",
    "        logger.info(\"Validation is done. Result images are saved to {}\".format(args.img_dir))\n",
    "    elif args.mode == 'cv-save':\n",
    "        save_dir = Path(args.img_dir) / \"cv_images_{}\".format(step)\n",
    "        logger.info(\"Save CV results to {} ...\".format(save_dir))\n",
    "        utils.rm(save_dir)\n",
    "        for tag, loader in val_loaders.items():\n",
    "            l1, ssim, msssim = evaluator.cross_validation(\n",
    "                gen, step, loader, tag, n_batches=evaluator.n_cv_batches, save_dir=(save_dir / tag)\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(args.mode)\n",
    "\n",
    "\n",
    "eval_ckpt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
